import numpy as np

np.random.seed(3)

from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.utils import shuffle
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB, BernoulliNB

from db_utils import open_database, close_database


def load_dataset(db, table, ids=None):
    connection, cursor = db

    x = []
    y = []

    if ids is None:
        query = cursor.execute("SELECT id FROM functions")
        ids = [i[0] for i in query.fetchall()]

    limit = 250
    for j in range(0, len(ids), limit):
        for k in range(j, j + limit):
            if k == len(ids):
                break

            # embedding
            fetched = cursor.execute("SELECT {} FROM {} WHERE id=?".format(table, table),
                                     (ids[k],)).fetchone()
            embedding = fetched[0]
            embedding = embedding[1:-1].split()
            embedding = [float(e) for e in embedding]

            # label
            fetched = cursor.execute("SELECT keyword FROM functions WHERE id=?", (ids[k],)).fetchone()
            keyword = fetched[0]

            x.append(embedding)
            y.append(keyword)

    scaler = StandardScaler()
    scaler.fit(x)
    x = scaler.transform(x)

    return shuffle(x, y)


def test_train_split(db, split_size):
    connection, cursor = db

    cursor.execute("SELECT DISTINCT keyword FROM functions")
    keywords = [i[0] for i in cursor.fetchall()]

    train, test = [], []

    for keyword in keywords:
        print(keyword)

        cursor.execute("SELECT count(*) FROM functions WHERE keyword='{}'".format(keyword))
        total = cursor.fetchone()[0]

        test_size = int(total * split_size)
        train_size = total - test_size

        print(total, test_size, train_size)

        cursor.execute("SELECT DISTINCT function_name FROM functions WHERE keyword='{}'".format(keyword))
        rows = cursor.fetchall()

        print("{} functions".format(len(rows)))

        np.random.shuffle(rows)

        def add(limit, start=0):
            l = []
            for i in range(start, len(rows)):
                cursor.execute("SELECT * FROM functions WHERE function_name='{}'".format(rows[i][0]))
                for f in cursor.fetchall():
                    l.append(f[0])
                if len(l) >= limit:
                    return l, i + 1

            return l, len(rows)

        train_functions, index = add(train_size)
        test_functions, index = add(test_size, index)

        train.extend(train_functions)
        test.extend(test_functions)

    x_train, y_train = load_dataset(db, table=table, ids=train)
    x_test, y_test = load_dataset(db, table=table, ids=test)

    return x_train, x_test, y_train, y_test


def knn(dataset):
    run_grid_search(dataset, KNeighborsClassifier(), parameters={
        'n_neighbors': range(1, 4),
        'weights': ['uniform', 'distance'],
        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
        'leaf_size': [10, 20, 30, 50, 100],
    })


def svm(dataset):
    Cs = [0.001, 0.01, 0.1, 1]
    gammas = [0.001, 0.01, 0.1]

    run_grid_search(dataset, SVC(kernel='rbf'), parameters={'C': Cs, 'gamma': gammas})
    run_grid_search(dataset, SVC(kernel='linear'), parameters={'C': Cs})
    run_grid_search(dataset, SVC(kernel='sigmoid'), parameters={'C': Cs, 'gamma': gammas})
    run_grid_search(dataset, SVC(kernel='poly'), parameters={'C': Cs, 'gamma': gammas, 'degree': range(2, 5)})


def logistic_regression(dataset):
    run_grid_search(dataset, LogisticRegression(), parameters=[
        {
            'penalty': ['l1', 'l2'],
            'C': [0.001, 0.01, 0.1, 1, 10, 100],
            'solver': ['liblinear', 'saga'],
            'multi_class': ['auto']
        },
        {
            'penalty': ['l2'],
            'max_iter': range(10, 500, 20),
            'solver': ['newton-cg', 'lbfgs', 'sag'],
            'multi_class': ['auto']
        },
        {
            'penalty': ['l2'],
            'max_iter': range(10, 500, 20),
            'solver': ['liblinear'],
            'multi_class': ['auto']
        }
    ])


def random_forest(dataset):
    run_grid_search(dataset, RandomForestClassifier(), parameters={
        'bootstrap': [False, True],
        'max_depth': [30, 60, 90],
        'min_samples_leaf': [1],
        'min_samples_split': [4, 5],
        'n_estimators': [200, 400, 600, 800]
    })


def decision_tree(dataset):
    run_grid_search(dataset, DecisionTreeClassifier(), parameters={
        'criterion': ['gini', 'entropy'],
        'splitter': ['random', 'best'],
        'max_depth': range(4, 21, 2),
    })


def sgd(dataset):
    run_grid_search(dataset, SGDClassifier(), parameters={
        'loss': ['hinge', 'perceptron', 'huber', 'epsilon_insensitive'],
        'penalty': ['elasticnet', 'l1', 'l2', None],
        'alpha': [1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1],
        'l1_ratio': [0, 0.05, 0.1, 0.2, 0.5, 0.8, 0.9, 0.95, 1],
    })


def naive_bayes(dataset):
    run_grid_search(dataset, GaussianNB(), parameters={
        'var_smoothing': [1e-09, 1e-08, 1e-05, 1e-03, 1e-02, 1e-01]
    })
    run_grid_search(dataset, BernoulliNB(), parameters={
        'alpha': [0.001, 0.01, 0.1, 1, 10, 100],
        'binarize': [0, 1, 10, 100],
        'fit_prior': [True, False]
    })


def run_grid_search(dataset, classifier, parameters=None, nfolds=5):
    if parameters is None:
        parameters = {}

    x_train, x_test, y_train, y_test = dataset

    grid_search = GridSearchCV(classifier, parameters, cv=nfolds, scoring='accuracy', verbose=2)
    grid_search.fit(x_train, y_train)

    classes = ["encoding", "encryption", "sorting", "string"]
    clf = grid_search.best_estimator_
    clf.fit(x_train, y_train)

    y_pred = clf.predict(x_test)

    print(grid_search.best_params_)
    print(classification_report(y_test, y_pred))


def main():
    db = open_database("data/embeddings.db")
    dataset = test_train_split(db, split_size=0.20)
    close_database(db)

    knn(dataset)
    svm(dataset)
    logistic_regression(dataset)
    random_forest(dataset)
    decision_tree(dataset)
    sgd(dataset)
    naive_bayes(dataset)


table = "embeddings_asm2vec"
# table = "embeddings_safe"

if __name__ == '__main__':
    main()
